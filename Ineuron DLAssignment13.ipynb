{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861fd0cf",
   "metadata": {},
   "source": [
    "1.\tWhat are the forward pass and backward pass of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617053a",
   "metadata": {},
   "source": [
    "Forward Propagation is the way to move from the Input layer (left) to the Output layer (right) in the neural network. The process of moving from the right to left i.e backward from the Output to the Input layer is called the Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d18e0",
   "metadata": {},
   "source": [
    "2.\tName three popular activation functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef53aa6",
   "metadata": {},
   "source": [
    "There are perhaps three activation functions you may want to consider for use in hidden layers; they are: Rectified Linear Activation (ReLU) Logistic (Sigmoid) Hyperbolic Tangent (Tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369962a",
   "metadata": {},
   "source": [
    "3. What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52cc6f",
   "metadata": {},
   "source": [
    "The difference is this: we fit the basis expansion, as well as the linear classifier. Fundamentally, backpropagation is just a special case of reverse-mode autodiff, applied to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb60df2",
   "metadata": {},
   "source": [
    "4.\tCan you list all the hyperparameters you can tweak in an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f02c6",
   "metadata": {},
   "source": [
    "In general, the hyperparameters of a neural network you can adjust are the number of hidden layers, the number of neurons in each hidden layer, and the activation function used by each neuron. For binary classification, use the logistic activation function. For a multi-class problem, use softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9716905",
   "metadata": {},
   "source": [
    "5. If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2e57c",
   "metadata": {},
   "source": [
    "If the MLP overfits the training data, how could you tweak these hyperparameters? If the MLP overfits the training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a68807",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9287da",
   "metadata": {},
   "source": [
    "1. How to tune the hyperparameters using cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f8211",
   "metadata": {},
   "source": [
    "Select the right type of model.\n",
    "Review the list of parameters of the model and build the HP space.\n",
    "Finding the methods for searching the hyperparameter space.\n",
    "Applying the cross-validation scheme approach.\n",
    "Assess the model score to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cdea3",
   "metadata": {},
   "source": [
    "2. Expalin Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8cb50",
   "metadata": {},
   "source": [
    "The Sigmoid function performs the role of an activation function in machine learning which is used to add non-linearity in a machine learning model. Basically, the function determines which value to pass as output and what not to pass as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812fe3a1",
   "metadata": {},
   "source": [
    "3. Explain Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38feb1b1",
   "metadata": {},
   "source": [
    "A rectified linear unit (ReLU) is an activation function that introduces the property of non-linearity to a deep learning model and solves the vanishing gradients issue. \"It interprets the positive part of its argument. It is one of the most popular activation functions in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15459888",
   "metadata": {},
   "source": [
    "4. Explain Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149d4ee",
   "metadata": {},
   "source": [
    "Tanh is the hyperbolic tangent function, which is the hyperbolic analogue of the Tan circular function used throughout trigonometry. Tanh[Î±] is defined as the ratio of the corresponding hyperbolic sine and hyperbolic cosine functions via ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9aaf7",
   "metadata": {},
   "source": [
    "5. Explain LeakyRelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0e999",
   "metadata": {},
   "source": [
    "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f6c26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d178cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e999c7b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5655c99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e8850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

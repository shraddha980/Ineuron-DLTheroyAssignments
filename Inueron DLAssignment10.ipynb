{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df8a173",
   "metadata": {},
   "source": [
    "1.\tWhat does a SavedModel contain? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160c924",
   "metadata": {},
   "source": [
    "A SavedModel contains a complete TensorFlow program, including trained parameters (i.e, tf. Variable s) and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying with TFLite, TensorFlow. js, TensorFlow Serving, or TensorFlow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c449c",
   "metadata": {},
   "source": [
    "2.\tWhen should you use TF Serving? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888d882",
   "metadata": {},
   "source": [
    "TF-Serving is actively maintained by TensorFlow, which means that its usage is recommended for the LTS (Long Time Support) they provide. Both the consistency and the ease to deploy ML models, makes TF-Serving a tool to consider in order to serve TensorFlow's models in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69187544",
   "metadata": {},
   "source": [
    "3.\tHow do you deploy a model across multiple TF Serving instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6f757",
   "metadata": {},
   "source": [
    "A) BASIC INSTALLATIONS:\n",
    "To use GPUs you cannot use garden variety docker, but will instead need to install a version of nvidia-docker (1 or 2). I strongly recommend nvidia-docker 2. Of note, nvidia-docker has been deprecated and is already having compatibility issues with TF versions etc. If you have nvidia-docker on your machine, you will need to remove it with the following code (If you donâ€™t have nvidia-docker already installed then skip to next block). \n",
    "\n",
    "B) MODEL SERVER FOR SINGLE MODEL\n",
    "To setup the server to serve a single model, enter the following command:\n",
    "\n",
    "\n",
    "docker run --runtime=nvidia -p 8501:8501 \\\n",
    " --mount type=bind,source=/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu,target=/models/half_plus_two \\\n",
    " -e MODEL_NAME=half_plus_two -t tensorflow/serving:latest-gpu &\n",
    "The above code can be understood as follows, piece-by-piece: It instructs for gpu usage (--runtime=nvidia) and opens up a port 8501 designated for RESTful messaging. It then binds the docker container's 8501 port to the host machine's 8501 port (-p 8501:8501). Next the location of the ML model on the host machine is bound to the location to which it will be copied onto in the docker container (--mount type=bind,source=<model location on host>,target=<model location in container>). Next the environmental variable representing the model name is explicitly changed to whatever name we've chosen to call our model (-e MODEL_NAME=<model name>). And finally we specify that we are running tensorFlow serving (-t tensorflow/serving:latest-gpu). Notably, each of the pieces in the code have default settings which we should be aware of. For instance, target location of model in docker container defaults to /models/model, i.e model_base_path = /models/model by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd7de8",
   "metadata": {},
   "source": [
    "4.\tWhat is quantization-aware training, and why would you need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1aa24",
   "metadata": {},
   "source": [
    "Quantization-aware training is a method that allows practitioners to apply quantization techniques without sacrificing accuracy. It is done in the model training process rather than after the fact. The model size can typically be reduced by two to four times, and sometimes even more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f144b",
   "metadata": {},
   "source": [
    "5.\tWhen training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088b768",
   "metadata": {},
   "source": [
    "MirroredStrategy is a method that you can use to perform synchronous distributed training across multiple GPUs. Using this method, you can create replicas of your model variables which are mirrored across your GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd99103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

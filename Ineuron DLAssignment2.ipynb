{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5234634b",
   "metadata": {},
   "source": [
    "1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b0d14",
   "metadata": {},
   "source": [
    "An ANN consists of layers made up of interconnected neurons that receive a set of inputs and a set of weights. It then does some mathematical manipulation and outputs the results as a set of “activations” that are similar to synapses in biological neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e0f34",
   "metadata": {},
   "source": [
    "2.\tWhat are the different types of activation functions popularly used? Explain each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9891b50",
   "metadata": {},
   "source": [
    "Binary Step Function\n",
    "\n",
    "In simple terms, Binary functions will activate the neuron only if the input to the step function is above the threshold value; otherwise deactivates its output is not considered for the next hidden layer.\n",
    "Limitations:\n",
    "\n",
    "Applicable for only binary classification; not multi-classification\n",
    "The gradient of the step function will be zero (Gradient is zero = Weights and bias won’t be updated in backpropagation)\n",
    "\n",
    "Linear activation is proportional to the input. The variable ‘a’ is any constant value.\n",
    "\n",
    "Linear Activation Functions have one advantage over binary activation. The gradient of this step function won’t be zero but the derivative of the step function won’t have ‘x’. Hence, only bias will be updated in backpropagation but weights will not be updated.\n",
    "\n",
    "If we use linear activation functions, no matter how many layers are there in the neural network, the last layer will be again a linear function of the first layer (linear combination of all the linear functions still a linear function).\n",
    "\n",
    "Limitations:\n",
    "\n",
    "The differential result is constant.\n",
    "All layers of the neural network turn into one layer\n",
    "Non-Linear Activation Functions\n",
    "\n",
    "Practical real-life problems are often nonlinear in nature. Therefore, these problems cannot be solved without non-linear functions. Non-Linear Activation Functions are essential to introduce the non-linearity into the network. Thus helps the neural networks to learn and model non-linear and complex data, such as images, video, audio, and high dimensionality datasets.\n",
    "\n",
    "10 commonly used Non-Linear Activation Functions\n",
    "Sigmoid Function\n",
    "Tanh Function\n",
    "ReLU\n",
    "Leaky ReLU\n",
    "Exponential ReLU\n",
    "Parametric ReLU\n",
    "SWISH\n",
    "Softmax\n",
    "Soft Plus\n",
    "MaxOut\n",
    "1. Sigmoid function\n",
    "Sigmoid function shrink the input values into values between 0 and 1.\n",
    "\n",
    "Advantages\n",
    "\n",
    "Smooth gradient, preventing “jumps” in output values.\n",
    "Sigmoid is S-shaped, ‘monotonic’ & ‘differential’ function.\n",
    "Output values range between 0 and 1, shrinks input of a neuron.\n",
    "Disadvantages\n",
    "\n",
    "Vanishing gradient — for extreme values of X, there is almost no change to the gradient. No gradients = No learning. This is called causing a vanishing gradient problem.\n",
    "Note: Vanishing gradient causes because sigmoid squeezes the inputs\n",
    "\n",
    "non-zero-centric function.This makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n",
    "Computationally expensive because of slow convergence due to exponential function\n",
    "2. Tanh function\n",
    "Tanh function is similar to the sigmoid function but this step function is symmetric around the origin (zero centric function). Tanh function ranges from -1 to 1. The Tanh function is defined as-\n",
    "\n",
    "Advantages\n",
    "\n",
    "Tanh is ‘monotonic’ & ‘differential’ function.\n",
    "zero centric function: makes it easier for the model to understand strongly negative, neutral, and strongly positive input values.\n",
    "Optimization is easier\n",
    "Disadvantages:\n",
    "\n",
    "Vanishing gradient problem.\n",
    "Computationally expensive; Slow convergence due to exponential function\n",
    "ReLU (Rectified Linear Unit)\n",
    "ReLU activation function activates neurons only if the input of the step function is more than 0; otherwise deactivates. ReLU is mathematically defined as:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "ReLU is ‘monotonic’ & ‘differential’ function.\n",
    "Computationally efficient — converges very quickly\n",
    "Disadvantages\n",
    "\n",
    "ReLU is non zero centric function\n",
    "The Dying ReLU problem — ReLU blocks the inputs for less than zero. Suppose, if the bias is very negative and the inputs are not positive enough to overcome the bias, the neurons will be dead. Dead neurons don’t learn.\n",
    "Leaky ReLU\n",
    "Unlike ReLU, Leaky ReLU allows a small constant slope for negative inputs, enabling backpropagation, even for negative input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00109039",
   "metadata": {},
   "source": [
    "3.\tExplain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe73281",
   "metadata": {},
   "source": [
    "Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. If the result of this addition is larger than a given threshold θ the neuron fires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c50d3",
   "metadata": {},
   "source": [
    "4.\tExplain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ab906",
   "metadata": {},
   "source": [
    "Multi layer perceptron (MLP) is a supplement of feed forward neural network. It consists of three types of layers—the input layer, output layer and hidden layer, as shown in Fig. 3. The input layer receives the input signal to be processed.The solution to this problem is to expand beyond the single-layer architecture by adding an additional layer of units without any direct access to the outside world, known as a hidden layer. This kind of architecture — shown in Figure 4 — is another feed-forward network known as a multilayer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ff1ba",
   "metadata": {},
   "source": [
    "5.\tWhat is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ad15c",
   "metadata": {},
   "source": [
    "ANNs are used in various applications, including parallel processing, adaptive learning, fault tolerance, and data storage distribution, text classification and categorization, paraphrase. Forecast, image processing. ANN approaches work on biological instincts and helps to solve all physical, real-world problems.\n",
    "\n",
    "Feedforward Neural Networks: This is the simplest type of ANN architecture, where the information flows in one direction from input to output. The layers are fully connected, meaning each neuron in a layer is connected to all the neurons in the next layer.\n",
    "\n",
    "Recurrent Neural Networks (RNNs): These networks have a “memory” component, where information can flow in cycles through the network. This allows the network to process sequences of data, such as time series or speech.\n",
    "\n",
    "Convolutional Neural Networks (CNNs): These networks are designed to process data with a grid-like topology, such as images. The layers consist of convolutional layers, which learn to detect specific features in the data, and pooling layers, which reduce the spatial dimensions of the data.\n",
    "\n",
    "Autoencoders: These are neural networks that are used for unsupervised learning. They consist of an encoder that maps the input data to a lower-dimensional representation and a decoder that maps the representation back to the original data.\n",
    "\n",
    "Generative Adversarial Networks (GANs): These are neural networks that are used for generative modeling. They consist of two parts: a generator that learns to generate new data samples, and a discriminator that learns to distinguish between real and generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676609c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

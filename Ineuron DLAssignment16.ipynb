{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6c9c3b",
   "metadata": {},
   "source": [
    "1. What happens when you increase or decrease the optimizer learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce438550",
   "metadata": {},
   "source": [
    "Generally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e59bb4",
   "metadata": {},
   "source": [
    "2. What happens when you increase the number of internal hidden neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789ae6f",
   "metadata": {},
   "source": [
    "If it has more numbers of hidden neurons, might have a large training error due to overfitting. An exceeding number of hidden neurons made on the network deepen the local minima problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba861cfb",
   "metadata": {},
   "source": [
    "3. What happens when you increase the size of batch computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ee1f1",
   "metadata": {},
   "source": [
    "The batch size can be understood as a trade-off between accuracy and speed. Large batch sizes can lead to faster training times but may result in lower accuracy and overfitting, while smaller batch sizes can provide better accuracy, but can be computationally expensive and time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51d1bc",
   "metadata": {},
   "source": [
    "4. Why we adopt regularization to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5cacc",
   "metadata": {},
   "source": [
    "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d8d0d",
   "metadata": {},
   "source": [
    "5. What are loss and cost functions in deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d54e8",
   "metadata": {},
   "source": [
    "A loss function, also known as a cost function, is a fundamental component in the domain of artificial intelligence and machine learning. It represents a specific mathematical function that quantifies the discrepancy between predicted values and actual ground-truth values in a given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a47382",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "655dc672",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3a7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
